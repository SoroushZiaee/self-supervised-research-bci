{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "# %reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 22), padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "\n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 11))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 2))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints.\n",
    "        self.fc1 = nn.Linear(4 * 4 * 99, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        if self.training:\n",
    "            x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        if self.training:\n",
    "            x = F.dropout(x, 0.5)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        if self.training:\n",
    "            x = F.dropout(x, 0.5)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # FC Layer\n",
    "        # x = x.view(-1, 4*2*7)\n",
    "        x = x.reshape((-1, 4 * 4 * 99))\n",
    "        # x = torch.sigmoid(self.fc1(x))\n",
    "\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = EEGNet().cuda(0)\n",
    "# print(net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)).cuda(0))))\n",
    "criterion = nn.CrossEntropyLoss()  # nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=1)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer, max_lr=0.001, steps_per_epoch=1, epochs=200\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseConv2d(torch.nn.Conv2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        depth_multiplier=1,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        out_channels = in_channels * depth_multiplier\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=in_channels,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConv2D(nn.Module):\n",
    "    \"\"\"https://github.com/seungjunlee96/Depthwise-Separable-Convolution_Pytorch/blob/master/DepthwiseSeparableConvolution/DepthwiseSeparableConvolution.py\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        depth_multiplier=1,\n",
    "        kernel_size=3,\n",
    "        padding=\"valid\",\n",
    "        bias=False,\n",
    "    ):\n",
    "        super(SeparableConv2D, self).__init__()\n",
    "        self.depthwise = DepthwiseConv2d(\n",
    "            in_channels, depth_multiplier, kernel_size, padding=padding, bias=bias\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EEGNetv2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=4,\n",
    "        channels=22,\n",
    "        dropout_rate=0.5,\n",
    "        kernel_length=32,\n",
    "        F1=8,\n",
    "        D=2,\n",
    "        F2=16,\n",
    "    ):\n",
    "        super(EEGNetv2, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, F1, (kernel_length, 1), padding=\"same\", bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(F1, False)\n",
    "        self.dwconv2 = DepthwiseConv2d(\n",
    "            in_channels=F1,\n",
    "            depth_multiplier=D,\n",
    "            kernel_size=(1, channels),  ## (1, num-channels)\n",
    "            stride=1,\n",
    "            padding=\"valid\",\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.batchnorm2 = nn.BatchNorm2d(2 * F1, False)\n",
    "        # act elu\n",
    "        self.pooling1 = nn.AvgPool2d((4, 1))\n",
    "        # dropout\n",
    "\n",
    "        # Layer 2\n",
    "        self.sepconv2 = SeparableConv2D(2 * F1, F2, 1, (16, 1), padding=\"same\")\n",
    "        self.batchnorm3 = nn.BatchNorm2d(F2, False)\n",
    "        # elu\n",
    "        self.pooling2 = nn.AvgPool2d((8, 1))\n",
    "        # dropout\n",
    "\n",
    "        # FC Layer\n",
    "        self.fc1 = nn.Linear(16 * 24 * 1, 4)\n",
    "\n",
    "    def _forward_emb(self, x):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pooling1(x)\n",
    "        if self.training:\n",
    "            x = F.dropout(x, self.dropout_rate)\n",
    "\n",
    "        x = self.sepconv2(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pooling2(x)\n",
    "        if self.training:\n",
    "            x = F.dropout(x, self.dropout_rate)\n",
    "\n",
    "        # FC Layer\n",
    "        x = x.reshape((-1, 16 * 24 * 1))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_emb(x)\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = EEGNetv2(\n",
    "    num_classes=4,\n",
    "    channels=22,\n",
    "    dropout_rate=0.1,\n",
    "    kernel_length=32,\n",
    "    F1=8,\n",
    "    D=2,\n",
    "    F2=16,\n",
    ").cuda(0)\n",
    "# print(net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)).cuda(0))))\n",
    "criterion = nn.CrossEntropyLoss()  # nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.2)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.001, steps_per_epoch=1, epochs=200\n",
    ")\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBEEGNetv2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super(MBEEGNetv2, self).__init__()\n",
    "        self.mbeegnet1 = EEGNetv2(\n",
    "            num_classes=4,\n",
    "            channels=22,\n",
    "            dropout_rate=0,\n",
    "            kernel_length=16,\n",
    "            F1=4,\n",
    "            D=2,\n",
    "            F2=16,\n",
    "        )\n",
    "        self.mbeegnet2 = EEGNetv2(\n",
    "            num_classes=4,\n",
    "            channels=22,\n",
    "            dropout_rate=0.1,\n",
    "            kernel_length=32,\n",
    "            F1=8,\n",
    "            D=2,\n",
    "            F2=16,\n",
    "        )\n",
    "        self.mbeegnet3 = EEGNetv2(\n",
    "            num_classes=4,\n",
    "            channels=22,\n",
    "            dropout_rate=0.2,\n",
    "            kernel_length=64,\n",
    "            F1=16,\n",
    "            D=2,\n",
    "            F2=16,\n",
    "        )\n",
    "\n",
    "        # FC Layer\n",
    "        self.fc1 = nn.Linear(16 * 24 * 3, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat(\n",
    "            (\n",
    "                self.mbeegnet1._forward_emb(x),\n",
    "                self.mbeegnet2._forward_emb(x),\n",
    "                self.mbeegnet3._forward_emb(x),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MBEEGNetv2().cuda(0)\n",
    "# print(net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)).cuda(0))))\n",
    "criterion = nn.CrossEntropyLoss()  # nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.001, steps_per_epoch=1, epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dl, params=[\"acc\"]):\n",
    "    results = []\n",
    "    predicted = []\n",
    "    Y = []\n",
    "\n",
    "    for batch in dl:\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.permute(\n",
    "            torch.vstack(list(map(lambda a: a.unsqueeze(0), inputs.values()))),\n",
    "            (1, 2, 3, 0),\n",
    "        )\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = inputs.cuda(0), labels.type(torch.LongTensor).cuda(0)\n",
    "\n",
    "        pred = model(inputs)\n",
    "\n",
    "        predicted.append(pred.cpu().detach())\n",
    "        Y.append(labels.cpu())\n",
    "\n",
    "    predicted = torch.cat(predicted, 0)\n",
    "    Y = torch.cat(Y, 0)\n",
    "\n",
    "    loss = criterion(predicted, Y)\n",
    "\n",
    "    predicted = predicted.numpy()\n",
    "    Y = Y.numpy()\n",
    "\n",
    "    for param in params:\n",
    "        if param == \"acc\":\n",
    "            results.append(accuracy_score(Y, np.argmax(predicted, axis=1)))\n",
    "        if param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted, multi_class=\"ovr\"))\n",
    "        if param == \"kappa\":\n",
    "            results.append(cohen_kappa_score(Y, np.argmax(predicted, axis=1)))\n",
    "        if param == \"recall\":\n",
    "            results.append(\n",
    "                recall_score(Y, np.argmax(predicted, axis=1), average=\"micro\")\n",
    "            )\n",
    "        if param == \"precision\":\n",
    "            results.append(\n",
    "                precision_score(Y, np.argmax(predicted, axis=1), average=\"micro\")\n",
    "            )\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(\n",
    "                Y, np.argmax(predicted, axis=1), average=\"micro\"\n",
    "            )\n",
    "            recall = recall_score(Y, np.argmax(predicted, axis=1), average=\"micro\")\n",
    "            results.append(2 * precision * recall / (precision + recall))\n",
    "\n",
    "    results.append(loss)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from pase_eeg.lit_modules.simple_classifier_lit import EEGBCIIV2aDataLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lit = EEGBCIIV2aDataLit(\n",
    "    data_path=\"/data/BCI_Competition_IV/\",\n",
    "    channels_config=\"../../configs/eeg_recording_standard/international_10_20_22.py\",\n",
    "    train_patients=[],\n",
    "    test_patients=[9],\n",
    "    batch_size=32,\n",
    "    leave_one_out=False,\n",
    "    transforms=[\n",
    "        {\n",
    "            \"class_path\": \"pase_eeg.data.transforms.ToTensor\",\n",
    "            \"init_args\": {\"device\": \"cpu\"},\n",
    "        },\n",
    "        # {\n",
    "        #     \"class_path\": \"pase_eeg.data.transforms.ZNorm\",\n",
    "        #     \"init_args\": {\"stats\": \"./bci_comp_iv2a_stats.pkl\", \"mode\": \"mean-std\"},\n",
    "        # },\n",
    "    ],\n",
    ")\n",
    "data_lit.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"test_acc\": [],\n",
    "    \"lr\": [],\n",
    "    \"train_kappa\": [],\n",
    "    \"test_kappa\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "    print(\"\\nEpoch \", epoch)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch in data_lit.train_dataloader():\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.permute(\n",
    "            torch.vstack(list(map(lambda a: a.unsqueeze(0), inputs.values()))),\n",
    "            (1, 2, 3, 0),\n",
    "        )\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = inputs.cuda(0), labels.type(torch.LongTensor).cuda(0)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    # print(optimizer.param_groups[0][\"lr\"])\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Validation accuracy\n",
    "    params = [\"acc\", \"kappa\", \"auc\", \"fmeasure\", \"loss\"]\n",
    "    print(params)\n",
    "    print(\"Training Loss \", running_loss)\n",
    "    tr = evaluate(net, data_lit.train_dataloader(), params)\n",
    "    print(\"Train - \", tr)\n",
    "    ev = evaluate(net, data_lit.val_dataloader(), params)\n",
    "    print(\"Validation - \", ev)\n",
    "    history[\"train_loss\"].append(tr[-1])\n",
    "    history[\"train_acc\"].append(tr[0])\n",
    "    history[\"train_kappa\"].append(tr[1])\n",
    "\n",
    "    history[\"test_loss\"].append(ev[-1])\n",
    "    history[\"test_acc\"].append(ev[0])\n",
    "    history[\"test_kappa\"].append(ev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def draw_2d_plot(shape, plotter, data, num=None):\n",
    "    fig, axs = plt.subplots(*shape, figsize=(15, 15))\n",
    "    for i, idx in enumerate(product(*[list(range(n)) for n in shape])):\n",
    "        if num is not None and i >= num:\n",
    "            break\n",
    "        plotter(axs[idx[0], idx[1]], data[i])\n",
    "\n",
    "\n",
    "def tuple_plotter(axes, data):\n",
    "    name, data = data\n",
    "    axes.title.set_text(name)\n",
    "    x = np.array([i for i in range(len(data))])\n",
    "    axes.plot(x, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(history[\"test_acc\"]), max(history[\"test_kappa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[\"test_acc\"][np.argmax(history[\"test_kappa\"])], history[\"test_kappa\"][\n",
    "    np.argmax(history[\"test_kappa\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [2, 2]\n",
    "num = 4\n",
    "data = list(history.items())\n",
    "draw_2d_plot(shape, tuple_plotter, data, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(history[\"lr\"]))], history[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "idxs = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "stat = []\n",
    "\n",
    "for i in idxs:\n",
    "    print(i)\n",
    "    # net = EEGNetv2(\n",
    "    #     num_classes=4,\n",
    "    #     channels=22,\n",
    "    #     dropout_rate=0.1,\n",
    "    #     kernel_length=32,\n",
    "    #     F1=8,\n",
    "    #     D=2,\n",
    "    #     F2=16,\n",
    "    # ).cuda(0)\n",
    "    net = MBEEGNetv2().cuda(0)\n",
    "    # print(net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)).cuda(0))))\n",
    "    criterion = nn.CrossEntropyLoss()  # nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), weight_decay=0.5)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.001, steps_per_epoch=1, epochs=200\n",
    "    )\n",
    "\n",
    "    data_lit = EEGBCIIV2aDataLit(\n",
    "        data_path=\"/data/BCI_Competition_IV/\",\n",
    "        channels_config=\"../../configs/eeg_recording_standard/international_10_20_22.py\",\n",
    "        train_patients=[],\n",
    "        test_patients=[i],\n",
    "        batch_size=32,\n",
    "        leave_one_out=False,\n",
    "        transforms=[\n",
    "            {\n",
    "                \"class_path\": \"pase_eeg.data.transforms.ToTensor\",\n",
    "                \"init_args\": {\"device\": \"cpu\"},\n",
    "            },\n",
    "            # {\n",
    "            #     \"class_path\": \"pase_eeg.data.transforms.ZNorm\",\n",
    "            #     \"init_args\": {\"stats\": \"./bci_comp_iv2a_stats.pkl\", \"mode\": \"mean-std\"},\n",
    "            # },\n",
    "        ],\n",
    "    )\n",
    "    data_lit.setup()\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"lr\": [],\n",
    "        \"train_kappa\": [],\n",
    "        \"test_kappa\": [],\n",
    "    }\n",
    "    for epoch in tqdm(range(200)):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for batch in data_lit.train_dataloader():\n",
    "            inputs, labels = batch\n",
    "            inputs = torch.permute(\n",
    "                torch.vstack(list(map(lambda a: a.unsqueeze(0), inputs.values()))),\n",
    "                (1, 2, 3, 0),\n",
    "            )\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = inputs.cuda(0), labels.type(torch.LongTensor).cuda(0)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        # print(optimizer.param_groups[0][\"lr\"])\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation accuracy\n",
    "        params = [\"acc\", \"kappa\", \"auc\", \"fmeasure\", \"loss\"]\n",
    "        # print(params)\n",
    "        # print(\"Training Loss \", running_loss)\n",
    "        tr = evaluate(net, data_lit.train_dataloader(), params)\n",
    "        # print(\"Train - \", tr)\n",
    "        ev = evaluate(net, data_lit.val_dataloader(), params)\n",
    "        # print(\"Validation - \", ev)\n",
    "        history[\"train_loss\"].append(tr[-1])\n",
    "        history[\"train_acc\"].append(tr[0])\n",
    "        history[\"train_kappa\"].append(tr[1])\n",
    "\n",
    "        history[\"test_loss\"].append(ev[-1])\n",
    "        history[\"test_acc\"].append(ev[0])\n",
    "        history[\"test_kappa\"].append(ev[1])\n",
    "    stat.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for his in stat:\n",
    "    print(max(his[\"test_acc\"]), max(his[\"test_kappa\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.conv1.weight.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
